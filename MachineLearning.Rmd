---
title: "Supervised Learning - Machine Learning"
author: "Xing Song"
date: "11/11/2020"
output: html_document
---

Let's do some preparation for the class by running the following chunk of codes

```{r setup}
#set up rmd 
knitr::opts_chunk$set(message=F,
                      warning=F,
                      echo=F,
                      fig.width=8,
                      fig.height=5)

#load packages
pacman::p_load(tidyverse,
               pROC,
               PRROC,
               party,
               # rpart,
               randomForest)

#load data
idd_bmi<-readRDS("./idd_bmi_final.rda") %>%
  mutate(DIAGNOSIS=as.factor(DIAGNOSIS),
         OSA_ind=as.factor(OSA_ind),
         sex_f_ind=as.factor(sex_f_ind),
         RACE=as.factor(RACE))
```

Let's recall the two logistic regression models we built last time to answer the research question:

> What are the risk factors for developing obstructive sleep apnea (OSA) among adult IDD patients? Can we build a predictive model for predicting likelihood of OSA among new IDD patients?


The one based on filter-based feature selection (univariate analysis):
```{r}
fit_reg1<-glm(OSA_ind~DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX,
              data=idd_bmi,
              family=binomial())

summary(fit_reg1)
```


The one based on wrapper-based feature selection (step-wise regression): 

```{r}
fit_reg2<-glm(OSA_ind~DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind,
              data=idd_bmi,
              family=binomial())

summary(fit_reg2)
```

************************************************************************************
************************************************************************************

##Decision Tree

Decision tree algorithm falls under the category of supervised learning. They can be used to solve both regression and classification problems. Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label and attributes are represented on the internal node of the tree. In addition, decision tree is one type of "embedded" feature selection model.

Let's first clarify some terms used with decision trees:

* **Root Node**: It represents entire population or sample and this further gets divided into two or more homogeneous sets
* **Splitting**: It is a process of dividing a node into two or more sub-nodes
* **Decision/Internal Node**: When a sub-node splits into further sub-nodes, then it is called decision/internal node
* **Terminal/Leaf Node**: Nodes do not split is called Leaf or Terminal node
* **Branch/Sub-Tree**: A sub section of entire tree is called branch or sub-tree
* **Parent and Child Node**: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node


```{r tree_model,fig.width=10,fig.height=6}
fit_tree<-ctree(OSA_ind ~ DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind + RACE,
                data=idd_bmi)

plot(fit_tree)
```

Each decision notes usually show:       

* splitting variable        
* splitting criteria      

Each terminal notes usually show:       

* number of training samples ended up in the node       
* distribution of the predicted class (0 = non-sleap-apnea; 1 = sleep-apnea)        

Let's compare the prediction result among the three models

```{r}
fit_reg1_roc<-pROC::roc(idd_bmi$OSA_ind,fit_reg1$fitted.values)
fit_reg2_roc<-pROC::roc(idd_bmi$OSA_ind,fit_reg2$fitted.values)

tree_pred<-matrix(unlist(predict(fit_tree,newdata=idd_bmi[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],type="prob")), nrow=nrow(idd_bmi), byrow=T)
fit_tree_roc<-pROC::roc(idd_bmi$OSA_ind,tree_pred[,2])

pROC::ggroc(list(Regression1=fit_reg1_roc,
                 Regression2=fit_reg2_roc,
                 Decision_Tree=fit_tree_roc))+
  geom_abline(intercept=1,linetype=2)+
  labs(subtitle = paste0("Regression 1 AUC:",round(fit_reg1_roc$auc,4),"\n",
                         "Regression 2 AUC:",round(fit_reg2_roc$auc,4),"\n",
                         "Decision-Tree AUC:",round(fit_tree_roc$auc,4)))
```

************************************************************************************
************************************************************************************

##Random Forest

Random Forest is **an ensemble** of multiple decision trees. An ensemble is simply a collection of models trained on the same task. An ensemble of different models that all achieve similar generalization performance often outperforms any of the individual models.

```{r fit_rf}
fit_rf10<-randomForest(OSA_ind ~ DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind + RACE, 
                       data=idd_bmi,ntree = 10, mtry=0.8, keep.forest = T, proximity = T, importance=T)

fit_rf50<-randomForest(OSA_ind ~ DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind + RACE, 
                       data=idd_bmi,ntree = 50, mtry=0.8, keep.forest = T, proximity = T, importance=T)

fit_rf200<-randomForest(OSA_ind ~ DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind + RACE, 
                        data=idd_bmi,ntree = 200, mtry=0.8, keep.forest = T, proximity = T, importance=T)
```


```{r}
rf10_pred<-predict(fit_rf10,
                   newdata=idd_bmi[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                   type="prob")
fit_rf10_roc<-pROC::roc(idd_bmi$OSA_ind,rf10_pred[,2])
#========================================

rf50_pred<-predict(fit_rf50,
                   newdata=idd_bmi[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                   type="prob")
fit_rf50_roc<-pROC::roc(idd_bmi$OSA_ind,rf50_pred[,2])
#========================================

rf200_pred<-predict(fit_rf200,
                    newdata=idd_bmi[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                    type="prob")
fit_rf200_roc<-pROC::roc(idd_bmi$OSA_ind,rf200_pred[,2])
#========================================

pROC::ggroc(list(Regression1=fit_reg1_roc,
                 Regression2=fit_reg2_roc,
                 Decision_Tree=fit_tree_roc,
                 Random_Forest10=fit_rf10_roc,
                 Random_Forest50=fit_rf50_roc,
                 Random_Forest200=fit_rf200_roc))+
  geom_abline(intercept=1,linetype=2)+
  labs(subtitle = paste0("Regression 1 AUC:",round(fit_reg1_roc$auc,4),"\n",
                         "Regression 2 AUC:",round(fit_reg2_roc$auc,4),"\n",
                         "Decision-Tree AUC:",round(fit_tree_roc$auc,4),"\n",
                         "Random-Forest(10) AUC:",round(fit_rf10_roc$auc,4),"\n",
                         "Random-Forest(50) AUC:",round(fit_rf50_roc$auc,4),"\n",
                         "Random-Forest(200) AUC:",round(fit_rf200_roc$auc,4)
                         ))
```

####Variable Importance Ranking

```{r,fig.width=8,fig.height=4}
varImpPlot(fit_rf200)
```

* Mean Decrease Accuracy: Mean Decrease in Accuracy is the average decrease in accuracy from not including the variable
    * Mean Decrease in Accuracy can provide low importance to other correlated features if one of them is given high importance       
    
* Mean Decrease Gini: Mean Decrease in Gini is the total decrease in node impurities from splitting on the variable, averaged over all trees         
    * Mean Decrease Gini can be biased towards categorical features which contain many categories       

******************************************************************************************
******************************************************************************************

##Model Validation

As introduced in the class last time, "validation" is a commonly-adopted approach to fairly evaluate how well your model fit within a research dataset ("internal validation") and generalize to other dataset ("external validation"). In practice, there are several approaches to perform validations:

* Hold-out sets (e.g. 70% training, 30% testing)
* Leave-one-out
* k-fold cross validation

In this demonstration, I will show the first approach (Hold-out), while the implementation of the other two approaches can be extended from the first approach.

```{r}
set.seed(124)
train_ind<-sample(c(TRUE,FALSE),size=nrow(idd_bmi),c(0.7,0.3),replace=T)
train<-idd_bmi[train_ind,]
test<-idd_bmi[!train_ind,]
```

Let's first use this validation strategy to train and test all the 6 models we just developed.

```{r reg_val}
fit_reg1_tr<-glm(OSA_ind~DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX,
                 data=train,
                 family=binomial())

fit_reg1_tr_roc<-pROC::roc(train$OSA_ind,
                           fit_reg1_tr$fitted.values)
fit_reg1_ts_roc<-pROC::roc(test$OSA_ind,
                           predict(fit_reg1_tr,
                                   newdata = test[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX')],
                                   type="response"))
#========================================

fit_reg2_tr<-glm(OSA_ind~DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind,
                 data=train,
                 family=binomial())

fit_reg2_tr_roc<-pROC::roc(train$OSA_ind,
                           fit_reg2_tr$fitted.values)
fit_reg2_ts_roc<-pROC::roc(test$OSA_ind,
                           predict(fit_reg2_tr,
                                   newdata = test[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind')],
                                   type="response"))
#========================================

pROC::ggroc(list(Regression1_tr=fit_reg1_tr_roc,
                 Regression1_ts=fit_reg1_ts_roc,
                 Regression2_tr=fit_reg2_tr_roc,
                 Regression2_ts=fit_reg2_ts_roc))+
  geom_abline(intercept=1,linetype=2)+
  labs(subtitle = paste0("Regression 1 Training AUC:",round(fit_reg1_tr_roc$auc,4),"\n",
                         "Regression 1 Testing AUC:",round(fit_reg1_ts_roc$auc,4),"\n",
                         "Regression 2 Training AUC:",round(fit_reg2_tr_roc$auc,4),"\n",
                         "Regression 2 Testing AUC:",round(fit_reg2_ts_roc$auc,4))
                         )
```


####Overfitting

* Problem when using large number of variables and few samples        
* Model seems correct because training set classified properly, but when used with independent data, model performs poorly           
* This is one reason simpler models are preferred             


```{r tree_val}
fit_tree<-ctree(OSA_ind ~ DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind + RACE,
                data=train)

tree_pred_tr<-matrix(unlist(predict(fit_tree,newdata=train[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],type="prob")), nrow=nrow(train), byrow=T)

tree_pred_ts<-matrix(unlist(predict(fit_tree,newdata=test[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],type="prob")), nrow=nrow(test), byrow=T)

fit_tree_tr_roc<-pROC::roc(train$OSA_ind,tree_pred_tr[,2])
fit_tree_ts_roc<-pROC::roc(test$OSA_ind,tree_pred_ts[,2])

pROC::ggroc(list(Decision_Tree_tr=fit_tree_tr_roc,
                 Decision_Tree_ts=fit_tree_ts_roc))+
  geom_abline(intercept=1,linetype=2)+
  labs(subtitle = paste0("Decision Tree Training AUC:",round(fit_tree_tr_roc$auc,4),"\n",
                         "Decision Tree Testing AUC:",round(fit_tree_ts_roc$auc,4)))

```


```{r rf_val}
fit_rf10<-randomForest(OSA_ind ~ DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind + RACE, 
                     data=train,ntree = 10, mtry=0.8, keep.forest = T, proximity = T, importance=T)
rf10_pred_tr<-predict(fit_rf10,
                      newdata=train[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                      type="prob")
rf10_pred_ts<-predict(fit_rf10,
                      newdata=test[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                      type="prob")

fit_rf10_tr_roc<-pROC::roc(train$OSA_ind,rf10_pred_tr[,2])
fit_rf10_ts_roc<-pROC::roc(test$OSA_ind,rf10_pred_ts[,2])
#=================================================

fit_rf50<-randomForest(OSA_ind ~ DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind + RACE, 
                     data=train,ntree = 50, mtry=0.8, keep.forest = T, proximity = T, importance=T)
rf50_pred_tr<-predict(fit_rf50,
                      newdata=train[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                      type="prob")
rf50_pred_ts<-predict(fit_rf50,
                      newdata=test[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                      type="prob")

fit_rf50_tr_roc<-pROC::roc(train$OSA_ind,rf50_pred_tr[,2])
fit_rf50_ts_roc<-pROC::roc(test$OSA_ind,rf50_pred_ts[,2])
#=================================================

fit_rf200<-randomForest(OSA_ind ~ DIAGNOSIS + bmi_median + AGE_AT_FIRSTDX + sex_f_ind + RACE, 
                     data=train,ntree = 200, mtry=0.8, keep.forest = T, proximity = T, importance=T)
rf200_pred_tr<-predict(fit_rf200,
                      newdata=train[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                      type="prob")
rf200_pred_ts<-predict(fit_rf200,
                      newdata=test[,c('DIAGNOSIS','bmi_median','AGE_AT_FIRSTDX','sex_f_ind','RACE')],
                      type="prob")

fit_rf200_tr_roc<-pROC::roc(train$OSA_ind,rf200_pred_tr[,2])
fit_rf200_ts_roc<-pROC::roc(test$OSA_ind,rf200_pred_ts[,2])
#=================================================

pROC::ggroc(list(Random_Forest10_tr=fit_rf10_tr_roc,
                 Random_Forest10_ts=fit_rf10_ts_roc,
                 Random_Forest50_tr=fit_rf50_tr_roc,
                 Random_Forest50_ts=fit_rf50_ts_roc,
                 Random_Forest200_tr=fit_rf200_tr_roc,
                 Random_Forest200_ts=fit_rf200_ts_roc))+
  geom_abline(intercept=1,linetype=2)+
  labs(subtitle = paste0("Random-Forest(10) Training AUC:",round(fit_rf10_tr_roc$auc,4),"\n",
                         "Random-Forest(10) Testing AUC:",round(fit_rf10_ts_roc$auc,4),"\n",
                         "Random-Forest(50) Training AUC:",round(fit_rf50_tr_roc$auc,4),"\n",
                         "Random-Forest(50) Testing AUC:",round(fit_rf50_ts_roc$auc,4),"\n",
                         "Random-Forest(200) Training AUC:",round(fit_rf200_tr_roc$auc,4),"\n",
                         "Random-Forest(200) Testing AUC:",round(fit_rf200_ts_roc$auc,4)
                         ))
```


************************************************************************************
************************************************************************************

Now, let's compare the testing/validation results among the 6 models:

```{r}
pROC::ggroc(list(Regression1=fit_reg1_ts_roc,
                 Regression2=fit_reg2_ts_roc,
                 Decision_Tree=fit_tree_ts_roc,
                 Random_Forest10=fit_rf10_ts_roc,
                 Random_Forest50=fit_rf50_ts_roc,
                 Random_Forest200=fit_rf200_ts_roc))+
  geom_abline(intercept=1,linetype=2)+
  labs(subtitle = paste0("Regression 1 Testing AUC:",round(fit_reg1_ts_roc$auc,4),"\n",
                         "Regression 2 Testing AUC:",round(fit_reg2_ts_roc$auc,4),"\n",
                         "Decision-Tree Testing AUC:",round(fit_tree_ts_roc$auc,4),"\n",
                         "Random-Forest(10) Testing AUC:",round(fit_rf10_ts_roc$auc,4),"\n",
                         "Random-Forest(50) Testing AUC:",round(fit_rf50_ts_roc$auc,4),"\n",
                         "Random-Forest(200) Testing AUC:",round(fit_rf200_ts_roc$auc,4)
                         ))
```

